# ОТЧЁТ ПО ПРОЕКТУ

## 1. Цель проекта

Разработка и внедрение on-prem AI-ассистента, использующего большие языковые модели (LLM) для ответов на вопросы пользователей на основе базы знаний Teamly, с интеграцией в платформу Just AI.
Все данные, модели и вычисления размещаются **внутри контура заказчика**, без передачи информации во внешние облака.

---

## 2. Общая архитектура решения

Решение состоит из следующих логических слоёв:

1. **Инфраструктурный слой**
   Сервер с GPU, драйверами, Docker и сетевыми ограничениями.

2. **LLM Inference слой**
   Запуск и обслуживание LLM через vLLM с использованием PyTorch и CUDA.

3. **Knowledge / RAG слой**
   Загрузка статей из Teamly, разбиение на чанки, векторизация, хранение и поиск в Qdrant.

4. **Backend / Orchestration слой**
   API, управляющее логикой запроса, retrieval, генерации ответа и fallback.

5. **Интеграционный слой**
   Связь с Teamly (API + webhooks) и Just AI.

6. **Security / Compliance слой**
   Контроль доступа, изоляция данных, логирование, соответствие требованиям ИБ.

---

## 3. Модульная структура проекта и задачи

---

### МОДУЛЬ M1. Инфраструктура и среда выполнения

**Описание**
Подготовка серверной среды для on-prem инференса LLM.

**Задачи**

* Подготовка сервера (Linux)
* Установка NVIDIA driver под конкретную модель GPU
* Проверка доступности GPU (`nvidia-smi`)
* Установка Docker / Docker Compose
* Настройка volume’ов для:

  * моделей
  * векторов
  * логов
* Базовые сетевые ограничения (по требованиям ИБ)

**Результат**

* Сервер готов к запуску GPU-нагруженных сервисов
* GPU видна контейнерам
* Среда стабильна для дальнейшей разработки

---

### МОДУЛЬ M2. LLM Inference слой (vLLM)

**Описание**
Разворачивание и настройка LLM для инференса.

**Задачи**

* Установка Python-окружения
* Установка PyTorch с поддержкой CUDA
* Установка и конфигурация vLLM
* Загрузка выбранной LLM (on-prem)
* Настройка:

  * dtype (FP16 / BF16 / INT8 при необходимости)
  * использование VRAM
  * batching
  * ограничения на контекст
* Проверка стабильности и latency
* Экспонирование LLM как сервиса (OpenAI-compatible API)

**Результат**

* LLM стабильно отвечает на запросы
* Контролируемое использование GPU
* Готовность к интеграции с backend

---

### МОДУЛЬ M3. Knowledge / RAG слой

**Описание**
Подготовка и обслуживание базы знаний для retrieval-augmented generation.

**Задачи**

* Интеграция с Teamly API для получения статей
* Реализация полной переиндексации статьи при:

  * create
  * publish
  * edit
* Разбиение статей на чанки (parent / child)
* Генерация embeddings
* Разворачивание Qdrant (docker, on-prem)
* Загрузка векторов и метаданных в Qdrant
* Реализация retrieval:

  * поиск релевантных чанков
  * сбор родительского контекста
* Реализация fallback:

  * возврат статьи без генерации
  * отказ от ответа при низкой уверенности

**Результат**

* Актуальная векторная база знаний
* Контролируемое качество retrieval
* Предсказуемое поведение при отсутствии данных

---

### МОДУЛЬ M4. Backend / Orchestration API

**Описание**
Центральный backend-сервис, управляющий логикой запросов.

**Задачи**

* Реализация API (FastAPI)
* Приём запроса от Just AI
* Оркестрация:

  * retrieval
  * сбор контекста
  * вызов LLM
* Формирование финального ответа
* Управление fallback-логикой
* Обработка ошибок и таймаутов
* Логирование запросов и ответов (по требованиям ИБ)

**Результат**

* Единая точка входа для AI-ассистента
* Управляемая бизнес-логика ответов

---

### МОДУЛЬ M5. Интеграции

#### Teamly

* Подписка на webhooks (create / publish / edit)
* Обработка событий
* Получение актуального контента через API

#### Just AI

* Интеграция по API
* Соблюдение форматов запросов и ответов
* Поддержка диалогового сценария (если требуется)

**Результат**

* Полная связка Teamly → AI → Just AI
* Автоматическое обновление базы знаний

---

### МОДУЛЬ M6. Security / Compliance

**Описание**
Обеспечение требований информационной безопасности.

**Задачи**

* Проверка отсутствия outbound traffic
* Ограничение сетевых доступов
* Настройка логирования
* Контроль доступа к сервисам
* Формальное подтверждение соответствия требованиям ИБ заказчика

**Результат**

* Система соответствует требованиям on-prem эксплуатации
* Данные не покидают контур заказчика

---

## 4. Состав команды и ставки

| Роль                              | Ставка | Срок участия |
| --------------------------------- | ------ | ------------ |
| **AI / LLM Architect & Engineer** | 1.0    | 4 недели     |
| Backend Developer (Teamly)        | 1.0    | 4 недели     |
| Backend Developer (Just AI)       | 1.0    | 4 недели     |
| DevOps / Network Engineer         | 0.25   | 1 неделя     |
| Security / InfoSec Engineer       | 0.25   | 1 неделя     |

---

## 5. Распределение ответственности

### AI / LLM Architect

* Архитектура решения
* LLM inference
* RAG
* Оптимизация качества и производительности

### Backend Developers

* API
* Интеграции
* Бизнес-логика

### DevOps

* Инфраструктура
* GPU и контейнеры

### InfoSec

* ИБ-контроль
* Формальное согласование

---

## 6. Сроки реализации

### Неделя 1

* Инфраструктура готова
* GPU работает
* LLM запущена

### Неделя 2

* Teamly ingestion
* Qdrant
* embeddings
* retrieval

### Неделя 3

* Just AI интеграция
* fallback
* оптимизация параметров

### Неделя 4

* ИБ
* приёмка
* документация
* ввод в эксплуатацию

**Итого:** 3–4 недели активной реализации.
